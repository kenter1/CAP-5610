{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression  Loss Functions: Squared Error and Binary Cross Entropy\n",
    "Formulas from https://github.com/schneider128k/machine_learning_course/blob/master/slides/logistic_regression.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = train_images_original.shape[0]\n",
    "m_test = test_images_original.shape[0]\n",
    "features = train_images_original.shape[1] * train_images_original.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten(m, features, images):\n",
    "    flattened_images = images.reshape((m, features))\n",
    "    flattened_images = flattened_images.astype('float32') /255\n",
    "    return flattened_images\n",
    "\n",
    "train_images = flatten(m_train,features,train_images_original)\n",
    "test_images  = flatten(m_test,features,test_images_original)\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[:][1].reshape(28,28))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleImage = train_images[:][1].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exampleImage.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(exampleImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cycle algorithm https://stackoverflow.com/questions/19113189/detecting-cycles-in-a-graph-using-dfs-2-different-approaches-and-whats-the-dif"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#unvisited\n",
    "def findAdjacentNodes(x,y,image,marked):\n",
    "    dx = [0,0,1,-1]\n",
    "    dy = [1,-1,0,0]\n",
    "    \n",
    "    nodes = []\n",
    "    \n",
    "    for i in range(4):\n",
    "        if(image[x+dx[i]][y+dy[i]] > 0 and marked[x+dx[i]][y+dy[i]] == 0):\n",
    "            nodes.append((x+dx[i],y+dy[i]))\n",
    "    \n",
    "    return nodes\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def findCycle(v, u, image, marked):\n",
    "    a, b = v\n",
    "    marked[a][b] = 1\n",
    "    \n",
    "    for c,d in findAdjacentNodes(a,b,image,marked):\n",
    "        if(marked[c][d] == 0):\n",
    "            marked[c][d] = 1\n",
    "            #marked should be passed ref???\n",
    "            findCycle((c,d),v,image,marked)\n",
    "        elif(v != u):\n",
    "            #print(True)\n",
    "            return True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def DFSCycle(x, y, image):\n",
    "    marked = np.zeros(image.shape[0]*image.shape[1]).reshape(image.shape[0],image.shape[1])\n",
    "    findCycle((x,y),(x,y), image, marked)\n",
    "    return marked"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#finding cycles will not work because it can make circles everywhere\n",
    "#unless the pixel are 1 pixel thick\n",
    "\n",
    "exampleImage = train_images[:][4].reshape(28,28)\n",
    "s,t= np.where(exampleImage>0)\n",
    "s = s[0]\n",
    "t = t[0]\n",
    "exampleImage[4][15]\n",
    "marked = np.zeros(exampleImage.shape[0]*exampleImage.shape[1]).reshape(exampleImage.shape[0],exampleImage.shape[1])\n",
    "print(\"{} {}\".format(s ,t))\n",
    "for i in findAdjacentNodes(s,t,exampleImage,marked):\n",
    "    a, b = i\n",
    "    print(\"{} {}\".format(a,b))\n",
    "    \n",
    "\n",
    "findCycle((s,t),(s,t),exampleImage,marked)\n",
    "plt.imshow(marked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels_original)\n",
    "test_labels = to_categorical(test_labels_original)\n",
    "\n",
    "train_labels.shape, test_labels.shape\n",
    "\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_Extra = np.zeros(m_train*28*28).reshape(train_images.shape)\n",
    "np.copyto(train_images_Extra, train_images)\n",
    "\n",
    "test_images_Extra = np.zeros(m_test*28*28).reshape(test_images.shape)\n",
    "np.copyto(test_images_Extra, test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratios(image):\n",
    "    image = image.reshape(image.shape[0],28*28)\n",
    "    return np.count_nonzero(image,axis=1)/(28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activated(image):\n",
    "    image = image.reshape(image.shape[0],28*28)\n",
    "    return np.count_nonzero(image,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_images_Extra = addFeature(test_images_Extra, ratios(test_images))\n",
    "#train_images_Extra = addFeature(train_images_Extra, ratios(train_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 784))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_Extra.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc169e83208>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[:][1].reshape(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Flood_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floodfill(picture,replacement = 255,target = 0, xPos = 0, yPos = 0):\n",
    "    #print(\"{} {}\".format(xPos, yPos))\n",
    "    if(target == replacement):\n",
    "        return picture\n",
    "    if(picture[xPos][yPos] != target):\n",
    "        return picture\n",
    "    picture[xPos][yPos] = replacement\n",
    "    floodfill(picture,replacement,target, (xPos + 1) % 28, yPos)\n",
    "    floodfill(picture,replacement,target, (xPos - 1) % 28, yPos)\n",
    "    floodfill(picture,replacement,target, xPos, (yPos + 1) % 28)\n",
    "    floodfill(picture,replacement,target, xPos, (yPos - 1) % 28)\n",
    "    \n",
    "    return picture\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp1 = np.zeros(train_images_original.shape[0]*28*28).reshape(train_images_original.shape)\n",
    "#np.copyto(temp1, train_images_original)\n",
    "#a = np.zeros(train_images_original.shape[0]*28*28).reshape(train_images_original.shape)\n",
    "#a.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "#np.vectorize(floodfill)(temp1)\n",
    "from multiprocessing.pool import ThreadPool\n",
    "pool = ThreadPool(4) \n",
    "results = pool.map(floodfill, temp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 413 ms, sys: 169 ms, total: 582 ms\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(processes=4) \n",
    "#should not affect train_images_original\n",
    "a = pool.map(floodfill, train_images_original)\n",
    "#pool.map(floodfill, temp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array(a)\n",
    "b = b.reshape(b.shape[0],b.shape[1]*b.shape[2])\n",
    "b = (activated(b) == 784).astype(\"int\")\n",
    "b = to_categorical(b)\n",
    "\n",
    "train_images_Extra = np.append(train_images_Extra,b,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 786)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_Extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62.8 ms, sys: 108 ms, total: 171 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from multiprocessing import Pool\n",
    "pool = Pool(processes=4) \n",
    "#should not affect train_images_original\n",
    "c = pool.map(floodfill, test_images_original)\n",
    "#pool.map(floodfill, temp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.array(c)\n",
    "d = d.reshape(d.shape[0],d.shape[1]*d.shape[2])\n",
    "d = (activated(d) == 784).astype(\"int\")\n",
    "d = to_categorical(d)\n",
    "\n",
    "test_images_Extra = np.append(test_images_Extra,d,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 786)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_Extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 1.3328 - acc: 0.6804 - val_loss: 0.8046 - val_acc: 0.8343\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.6521 - acc: 0.8500 - val_loss: 0.5041 - val_acc: 0.8791\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.4683 - acc: 0.8797 - val_loss: 0.4015 - val_acc: 0.8964\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3945 - acc: 0.8951 - val_loss: 0.3554 - val_acc: 0.9045\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 5us/step - loss: 0.3571 - acc: 0.9027 - val_loss: 0.3292 - val_acc: 0.9106\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "output = train_labels.shape[1]\n",
    "inputSize = features\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(Dense(output, input_dim=inputSize, activation='softmax')) \n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "history = network.fit(train_images, \n",
    "                      train_labels, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=1000, \n",
    "                      validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = network.predict(test_images)\n",
    "\n",
    "predicted_labels = np.argmax(predictions, axis=1) \n",
    "predicted_labels = predicted_labels.astype('uint8')\n",
    "\n",
    "wrong_indices = [i for i in range(10000) if predicted_labels[i] != test_labels_original[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "wrong_index = wrong_indices[i]  \n",
    "predicted_labels[wrong_index], test_labels_original[wrong_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test_images_original[wrong_index])\n",
    "plt.grid(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 0s 8us/step - loss: 1.2815 - acc: 0.6973 - val_loss: 0.7669 - val_acc: 0.8449\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 0.6199 - acc: 0.8602 - val_loss: 0.4759 - val_acc: 0.8883\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 0.4391 - acc: 0.8888 - val_loss: 0.3741 - val_acc: 0.9064\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 0s 6us/step - loss: 0.3651 - acc: 0.9039 - val_loss: 0.3261 - val_acc: 0.9156\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 0s 7us/step - loss: 0.3261 - acc: 0.9118 - val_loss: 0.3004 - val_acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "output = train_labels.shape[1]\n",
    "inputSize = features\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(Dense(output, input_dim=inputSize + 2, activation='softmax')) \n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "history = network.fit(train_images_Extra, \n",
    "                      train_labels, \n",
    "                      epochs=epochs, \n",
    "                      batch_size=1000, \n",
    "                      validation_data=(test_images_Extra, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
