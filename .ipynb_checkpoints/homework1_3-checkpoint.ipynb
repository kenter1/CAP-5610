{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Implementation formulas from https://www.ics.uci.edu/~pjsadows/notes.pdf\n",
    "\n",
    "In this implementation we use a bias node instead of the + b in the linear equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = train_images_original.shape[0]\n",
    "m_test = test_images_original.shape[0]\n",
    "pixels = train_images_original.shape[1] * train_images_original.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flattening Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I made the top left to always activate as a bias node\n",
    "train_images = train_images_original.reshape((60000, 28 * 28))\n",
    "train_images[:,0] = 255\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images_original.reshape((10000, 28 * 28))\n",
    "test_images[:,0] = 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[:,][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7faf836189b0>,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[:,][5].reshape(28,28)), \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels_original)\n",
    "test_labels = to_categorical(test_labels_original)\n",
    "\n",
    "train_labels = train_labels.reshape(60000, 10)\n",
    "test_labels = test_labels.reshape(10000, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWeights(k, n):\n",
    "    return np.zeros(n*k).reshape(k,n)\n",
    "\n",
    "def generateRandomVector(k,n):\n",
    "    return np.random.rand(n*k).reshape(k,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 784), (784,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generateWeights(10, 784).shape, generateRandomVector(10,784)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backPropSM(X,y,W,target,batch_size):\n",
    "    m = X.shape[0]\n",
    "    ds = y - target\n",
    "    dw = np.dot(ds.T, X)\n",
    "    \n",
    "    return dw/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z is a vector of n size = 10 (number of nodes in the last layer represents (0-9 activation nodes))\n",
    "def softmax(s):\n",
    "    y = np.exp(s) / np.sum(np.exp(s))\n",
    "    #print(\"{}\".format(y.sum()))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPropSM(X, W, batch_size):\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "    y = []\n",
    "    #print(\"{} {}\".format(w.shape, X.shape))\n",
    "\n",
    "    for i in range(0,m):\n",
    "        s = (np.dot(W,X[i]))\n",
    "        a = softmax(s)\n",
    "        #print(\"{}\".format(a.sum()))\n",
    "        #print(\"{}\".format(a.shape))\n",
    "        y.append(a)\n",
    "        \n",
    "    \n",
    "    #a should be (10,m)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = forwardPropSM2(test_images,trainedWeights, test_images.shape[0])\n",
    "#y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does not work \n",
    "def forwardPropSM2(X, W, batch_size):\n",
    "    m = batch_size\n",
    "    n = X.shape[1] \n",
    "    #print(\"{} {}\".format(w.shape, X.shape))\n",
    "    s = (np.dot(X,W.T))\n",
    "    y = softmax(s)\n",
    "  \n",
    "    #a should be (10,m)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mini_batch_sm(epochs, lr, batch_size, X, t):\n",
    "    n = train_images.shape[1]\n",
    "    m = train_images.shape[0]\n",
    "    weights = generateWeights(10, 784)\n",
    "    for epoch in range(epochs):\n",
    "        shuffled_indices = np.random.permutation(m)\n",
    "        train_images_shuffled = X[shuffled_indices]\n",
    "        train_labels_shuffled = t[shuffled_indices]\n",
    "        for e in range(0,m,batch_size):    \n",
    "            xi = train_images_shuffled[e:e+batch_size]\n",
    "            ti = train_labels_shuffled[e:e+batch_size]\n",
    "\n",
    "            y = forwardPropSM(xi,weights,batch_size)\n",
    "            dw = backPropSM(xi,y,weights,ti,batch_size)\n",
    "            weights = weights - lr * dw  \n",
    "        print(\"Finished Epoch {}\".format(epoch))\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 0\n",
      "Finished Epoch 1\n",
      "Finished Epoch 2\n",
      "Finished Epoch 3\n",
      "Finished Epoch 4\n",
      "CPU times: user 8.35 s, sys: 8.92 s, total: 17.3 s\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 5\n",
    "lr = .5\n",
    "batch_size = 500\n",
    "trainedWeights = train_mini_batch_sm(epochs,lr,batch_size,train_images,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedWeights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score 9174/10000 : Accuracy 91.74%\n"
     ]
    }
   ],
   "source": [
    "predicted = forwardPropSM(test_images,trainedWeights, test_images.shape[0])\n",
    "df = pd.DataFrame(predicted)\n",
    "df = df.T\n",
    "pred_labels = df.idxmax()\n",
    "score = (pred_labels.values == test_labels_original).sum()\n",
    "print(\"Test Score {}/{} : Accuracy {}%\".format(score, test_labels_original.shape[0], 100*score/test_labels_original.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score 54917/60000 : Accuracy 91.52833333333334%\n"
     ]
    }
   ],
   "source": [
    "predicted = forwardPropSM(train_images,trainedWeights, train_images.shape[0])\n",
    "df = pd.DataFrame(predicted)\n",
    "df = df.T\n",
    "pred_labels = df.idxmax()\n",
    "score = (pred_labels.values == train_labels_original).sum()\n",
    "print(\"Train Score {}/{} : Accuracy {}%\".format(score, train_labels_original.shape[0], 100*score/train_labels_original.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>59990</th>\n",
       "      <th>59991</th>\n",
       "      <th>59992</th>\n",
       "      <th>59993</th>\n",
       "      <th>59994</th>\n",
       "      <th>59995</th>\n",
       "      <th>59996</th>\n",
       "      <th>59997</th>\n",
       "      <th>59998</th>\n",
       "      <th>59999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.004448</td>\n",
       "      <td>9.993364e-01</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>7.319772e-09</td>\n",
       "      <td>9.207924e-05</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.022238</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>1.298837e-04</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.038036</td>\n",
       "      <td>0.061263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000052</td>\n",
       "      <td>4.944697e-09</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.961175</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>9.955506e-01</td>\n",
       "      <td>1.313064e-07</td>\n",
       "      <td>0.985461</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.913071</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>1.325476e-05</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004651</td>\n",
       "      <td>3.058382e-05</td>\n",
       "      <td>0.020079</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.978692</td>\n",
       "      <td>2.212200e-04</td>\n",
       "      <td>8.668177e-04</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.963386</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.006254</td>\n",
       "      <td>0.027442</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>6.016804e-04</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.014074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321020</td>\n",
       "      <td>4.144978e-05</td>\n",
       "      <td>0.076153</td>\n",
       "      <td>0.002722</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>3.372199e-03</td>\n",
       "      <td>9.937346e-01</td>\n",
       "      <td>0.005550</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.012329</td>\n",
       "      <td>0.010760</td>\n",
       "      <td>0.016261</td>\n",
       "      <td>9.872609e-01</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.001917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>2.574033e-08</td>\n",
       "      <td>0.856416</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.089956</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>5.852591e-06</td>\n",
       "      <td>1.966798e-06</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.997429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>0.003809</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4.541630e-06</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.011439</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.654395</td>\n",
       "      <td>4.981170e-04</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>4.363409e-05</td>\n",
       "      <td>2.465957e-04</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.010477</td>\n",
       "      <td>0.931789</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>8.608860e-04</td>\n",
       "      <td>0.971741</td>\n",
       "      <td>0.039328</td>\n",
       "      <td>0.027765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000895</td>\n",
       "      <td>8.384048e-06</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>2.551157e-05</td>\n",
       "      <td>8.139470e-08</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.023125</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.019818</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>3.138084e-07</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.896493</td>\n",
       "      <td>0.000973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.006455</td>\n",
       "      <td>1.310902e-05</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.028644</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>1.598086e-05</td>\n",
       "      <td>3.819584e-06</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021462</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.180990</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.005733</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>8.401990e-07</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.002204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.006833</td>\n",
       "      <td>6.644040e-05</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.012618</td>\n",
       "      <td>0.010090</td>\n",
       "      <td>6.948281e-04</td>\n",
       "      <td>4.766916e-03</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003733</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>0.010793</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.979336</td>\n",
       "      <td>1.105796e-02</td>\n",
       "      <td>0.017239</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.813045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001244</td>\n",
       "      <td>5.517397e-06</td>\n",
       "      <td>0.028962</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.867866</td>\n",
       "      <td>0.007640</td>\n",
       "      <td>7.012063e-05</td>\n",
       "      <td>2.869941e-04</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.966535</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.782731</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>6.969083e-05</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.078440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 60000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0             1         2         3         4         5      \\\n",
       "0  0.004448  9.993364e-01  0.000788  0.000018  0.000010  0.000579   \n",
       "1  0.000052  4.944697e-09  0.000221  0.961175  0.000292  0.000002   \n",
       "2  0.004651  3.058382e-05  0.020079  0.014383  0.000034  0.978692   \n",
       "3  0.321020  4.144978e-05  0.076153  0.002722  0.000105  0.001880   \n",
       "4  0.000006  2.574033e-08  0.856416  0.000116  0.089956  0.000023   \n",
       "5  0.654395  4.981170e-04  0.001005  0.000250  0.000369  0.000727   \n",
       "6  0.000895  8.384048e-06  0.004836  0.000175  0.000107  0.000045   \n",
       "7  0.006455  1.310902e-05  0.006454  0.000581  0.028644  0.000321   \n",
       "8  0.006833  6.644040e-05  0.005087  0.020472  0.012618  0.010090   \n",
       "9  0.001244  5.517397e-06  0.028962  0.000108  0.867866  0.007640   \n",
       "\n",
       "          6             7         8         9        ...        59990  \\\n",
       "0  7.319772e-09  9.207924e-05  0.000002  0.000010    ...     0.000118   \n",
       "1  9.955506e-01  1.313064e-07  0.985461  0.000001    ...     0.000017   \n",
       "2  2.212200e-04  8.668177e-04  0.000441  0.000152    ...     0.000044   \n",
       "3  3.372199e-03  9.937346e-01  0.005550  0.000007    ...     0.000321   \n",
       "4  5.852591e-06  1.966798e-06  0.000070  0.997429    ...     0.006779   \n",
       "5  4.363409e-05  2.465957e-04  0.001371  0.000518    ...     0.000891   \n",
       "6  2.551157e-05  8.139470e-08  0.000290  0.001039    ...     0.000101   \n",
       "7  1.598086e-05  3.819584e-06  0.000631  0.000027    ...     0.021462   \n",
       "8  6.948281e-04  4.766916e-03  0.004814  0.000633    ...     0.003733   \n",
       "9  7.012063e-05  2.869941e-04  0.001370  0.000183    ...     0.966535   \n",
       "\n",
       "      59991     59992     59993     59994     59995         59996     59997  \\\n",
       "0  0.002113  0.000827  0.022238  0.000219  0.000040  1.298837e-04  0.000168   \n",
       "1  0.000048  0.000317  0.000438  0.913071  0.000333  1.325476e-05  0.000045   \n",
       "2  0.963386  0.000410  0.006254  0.027442  0.001232  6.016804e-04  0.000005   \n",
       "3  0.000058  0.005912  0.012329  0.010760  0.016261  9.872609e-01  0.005741   \n",
       "4  0.003138  0.007376  0.003809  0.003588  0.000045  4.541630e-06  0.001193   \n",
       "5  0.000537  0.010477  0.931789  0.004962  0.002592  8.608860e-04  0.971741   \n",
       "6  0.023125  0.000168  0.019818  0.001076  0.000008  3.138084e-07  0.000030   \n",
       "7  0.000008  0.180990  0.000319  0.005733  0.000004  8.401990e-07  0.000418   \n",
       "8  0.007538  0.010793  0.002446  0.031894  0.979336  1.105796e-02  0.017239   \n",
       "9  0.000049  0.782731  0.000561  0.001256  0.000147  6.969083e-05  0.003421   \n",
       "\n",
       "      59998     59999  \n",
       "0  0.038036  0.061263  \n",
       "1  0.000030  0.000119  \n",
       "2  0.009620  0.014074  \n",
       "3  0.001720  0.001917  \n",
       "4  0.011439  0.000201  \n",
       "5  0.039328  0.027765  \n",
       "6  0.896493  0.000973  \n",
       "7  0.001155  0.002204  \n",
       "8  0.000801  0.813045  \n",
       "9  0.001377  0.078440  \n",
       "\n",
       "[10 rows x 60000 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
